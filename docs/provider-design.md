# ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­è¨ˆæ›¸

## æ¦‚è¦

vibe-local-go ã‚’ Ollama å°‚ç”¨ã‹ã‚‰ã€è¤‡æ•°ã® LLM ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«å¯¾å¿œã•ã›ã‚‹ã€‚
ãƒ­ãƒ¼ã‚«ãƒ«å„ªå…ˆã®æ€æƒ³ã‚’ç¶­æŒã—ã¤ã¤ã€ã‚¯ãƒ©ã‚¦ãƒ‰ LLM ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚„é¸æŠè‚¢ã¨ã—ã¦ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

## å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¸€è¦§

### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

| ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | APIå½¢å¼ | ãƒ¢ãƒ‡ãƒ«ç®¡ç† | ç‰¹å¾´ |
|---|---|---|---|
| **Ollama** | OpenAIäº’æ› `/v1/chat/completions` + ç‹¬è‡ªAPI (`/api/tags`, `/api/pull`) | ã‚ã‚Šï¼ˆpull/listï¼‰ | æœ€ã‚‚ç°¡å˜ã€‚è‡ªå‹•ãƒ¢ãƒ‡ãƒ«DLå¯¾å¿œ |
| **llama-server** (llama.cpp) | OpenAIäº’æ› `/v1/chat/completions` | ãªã—ï¼ˆèµ·å‹•æ™‚ã«æŒ‡å®šæ¸ˆï¼‰ | è»½é‡ãƒ»é«˜é€Ÿã€‚KV cacheåˆ¶å¾¡å¯èƒ½ |
| **llama.app** (macOS) | llama-server ã¨åŒã˜ï¼ˆå†…éƒ¨ã§llama-serverèµ·å‹•ï¼‰ | GUIã§ç®¡ç† | macOSãƒã‚¤ãƒ†ã‚£ãƒ– |
| **LM Studio** | OpenAIäº’æ› `/v1/chat/completions` | GUIã§ç®¡ç† | Windows/Mac GUIã€‚ãƒãƒ¼ãƒˆ1234 |
| **LocalAI** | OpenAIäº’æ› | ã‚ã‚Š | Docker ãƒ™ãƒ¼ã‚¹ |

### ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

| ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | APIå½¢å¼ | ç‰¹å¾´ |
|---|---|---|
| **OpenAI** | OpenAI API | GPT-4o, o1 ç­‰ã€‚Function calling å®Œå…¨å¯¾å¿œ |
| **Anthropic** | Anthropic Messages API | Claudeã€‚ç‹¬è‡ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆtool_use blockï¼‰ |
| **Google Gemini** | OpenAIäº’æ› or Gemini API | Gemini 2.xã€‚OpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚ã‚Š |
| **GLM / CodeGeeX** (æ™ºè°±AI) | OpenAIäº’æ› | GLM-4, CodeGeeXã€‚ä¸­å›½ç™ºã€‚ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¼·ã„ |
| **DeepSeek** | OpenAIäº’æ› | DeepSeek Coder V2ã€‚ã‚³ã‚¹ãƒ‘æ¥µã‚ã¦é«˜ã„ |
| **Groq** | OpenAIäº’æ› | è¶…é«˜é€Ÿæ¨è«–ã€‚Llama/Mixtral |
| **Together AI** | OpenAIäº’æ› | OSS ãƒ¢ãƒ‡ãƒ«å¤šæ•°ã€‚å®‰ã„ |
| **OpenRouter** | OpenAIäº’æ› | å…¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çµ±åˆã‚²ãƒ¼ãƒˆã‚¦ã‚§ã‚¤ |

### å…±é€šç‚¹ã®ç™ºè¦‹

**ã»ã¼å…¨å“¡ãŒ OpenAI äº’æ› API ã‚’æŒã£ã¦ã„ã‚‹ã€‚** Anthropic ã ã‘ãŒç‹¬è‡ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‚

ã“ã‚Œã¯è¨­è¨ˆä¸Šã®å¤§ããªãƒ¡ãƒªãƒƒãƒˆï¼š
- OpenAIäº’æ›ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ1ã¤ã§ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰å•ã‚ãšå¤§åŠã‚’ã‚«ãƒãƒ¼ã§ãã‚‹
- ç‰¹æ®Šå¯¾å¿œãŒå¿…è¦ãªã®ã¯ Anthropic ã®ã¿

---

## ç¾çŠ¶ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨å•é¡Œç‚¹

```
ç¾åœ¨:
Agent â†’ *llm.Client (Ollamaå°‚ç”¨ãƒ»å…·è±¡å‹) â†’ Ollama HTTP API

å•é¡Œ:
1. Agent ãŒ *llm.Client ã«ç›´æ¥ä¾å­˜ï¼ˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãªã—ï¼‰
2. Ollama å›ºæœ‰ã® API (CheckModel, PullModel) ãŒã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«æ··åœ¨
3. Config ãŒ OllamaHost ã—ã‹æŒãŸãªã„
4. main.go ãŒ Ollama å‰æã®ãƒ•ãƒ­ãƒ¼ (checkOllamaConnection, pullModelIfNeeded)
5. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒ "Ollama error" ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
```

---

## è¨­è¨ˆæ–¹é‡

### åŸå‰‡

1. **OpenAIäº’æ›ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ã™ã‚‹** â€” å¤§åŠã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã“ã‚Œã§å‹•ã
2. **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰æ©Ÿèƒ½ã¯æ‹¡å¼µã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§** â€” ãƒ¢ãƒ‡ãƒ«ç®¡ç†ç­‰ã¯ Optional
3. **åˆå¿ƒè€…å‘ã‘è‡ªå‹•æ¤œå‡º** â€” è¨­å®šãªã—ã§ã‚‚å‹•ãã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½•ãŒå‹•ã„ã¦ã‚‹ã‹è‡ªå‹•æ¤œçŸ¥
4. **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³** â€” ãƒ¡ã‚¤ãƒ³å¤±æ•—æ™‚ã«æ¬¡ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¸è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ

### ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€ 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Agent Layer                               â”‚
â”‚  agent.go: LLMProvider ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ã¿çŸ¥ã‚‹                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Provider Router                                â”‚
â”‚  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ¶å¾¡ / ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚¹ / ãƒªãƒˆãƒ©ã‚¤                     â”‚
â”‚  Chain: [Local Main] â†’ [Local Sub] â†’ [Cloud Fallback]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI Compatibleâ”‚ â”‚ OpenAI Compatibleâ”‚ â”‚   Anthropic     â”‚
â”‚   Provider      â”‚ â”‚   Provider      â”‚ â”‚   Provider      â”‚
â”‚                 â”‚ â”‚                 â”‚ â”‚                 â”‚
â”‚ - Ollama        â”‚ â”‚ - llama-server  â”‚ â”‚ - Claude API    â”‚
â”‚ - LM Studio    â”‚ â”‚ - OpenAI        â”‚ â”‚                 â”‚
â”‚ - GLM/CodeGeeX â”‚ â”‚ - DeepSeek      â”‚ â”‚ (ç‹¬è‡ªå®Ÿè£…)       â”‚
â”‚ - Groq         â”‚ â”‚ - Together      â”‚ â”‚                 â”‚
â”‚ - OpenRouter   â”‚ â”‚ - Gemini        â”‚ â”‚                 â”‚
â”‚ - LocalAI      â”‚ â”‚                 â”‚ â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                    â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
  â”‚ Model Mgmt  â”‚     â”‚  (ãªã—)    â”‚      â”‚  (ãªã—)    â”‚
  â”‚ Extension   â”‚     â”‚            â”‚      â”‚            â”‚
  â”‚ - Pull      â”‚     â”‚            â”‚      â”‚            â”‚
  â”‚ - List      â”‚     â”‚            â”‚      â”‚            â”‚
  â”‚ - Search    â”‚     â”‚            â”‚      â”‚            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­è¨ˆ

### ã‚³ã‚¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

```go
package llm

// LLMProvider - å…¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒå®Ÿè£…ã™ã‚‹æœ€å°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
type LLMProvider interface {
    // Chat åŒæœŸãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆãƒ„ãƒ¼ãƒ«ä½¿ç”¨æ™‚ï¼‰
    Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error)

    // ChatStream ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆï¼ˆå¯¾è©±æ™‚ï¼‰
    ChatStream(ctx context.Context, req *ChatRequest) (<-chan StreamEvent, error)

    // CheckHealth ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç”Ÿå­˜ç¢ºèª
    CheckHealth(ctx context.Context) error

    // Info ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±
    Info() ProviderInfo
}

// ProviderInfo ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ¡ã‚¿æƒ…å ±
type ProviderInfo struct {
    Name        string       // "ollama", "llama-server", "openai", "anthropic", etc.
    Type        ProviderType // Local or Cloud
    BaseURL     string       // æ¥ç¶šå…ˆ
    Model       string       // ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«å
    Features    Features     // å¯¾å¿œæ©Ÿèƒ½ãƒ•ãƒ©ã‚°
}

type ProviderType string

const (
    ProviderTypeLocal ProviderType = "local"
    ProviderTypeCloud ProviderType = "cloud"
)

// Features ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¯¾å¿œæ©Ÿèƒ½
type Features struct {
    NativeFunctionCalling bool // true: OpenAIå¼tool_callså¯¾å¿œ
    ModelManagement       bool // true: ãƒ¢ãƒ‡ãƒ«DL/ä¸€è¦§ãŒå¯èƒ½
    Streaming             bool // true: SSEã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ
    Vision                bool // true: ç”»åƒå…¥åŠ›å¯¾å¿œ
    ContextWindowReport   bool // true: ãƒ¢ãƒ‡ãƒ«ã®ctx sizeã‚’è¿”ã›ã‚‹
}
```

### æ‹¡å¼µã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆOptionalï¼‰

```go
// ModelManager ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãŒã§ãã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç”¨ï¼ˆOllamaç­‰ï¼‰
type ModelManager interface {
    ListModels(ctx context.Context) ([]ModelInfo, error)
    PullModel(ctx context.Context, name string) error
    SearchModels(ctx context.Context, query string) ([]ModelInfo, error)
    DeleteModel(ctx context.Context, name string) error
}

// ModelInfo ãƒ¢ãƒ‡ãƒ«æƒ…å ±
type ModelInfo struct {
    Name         string
    Size         int64   // ãƒã‚¤ãƒˆ
    ContextSize  int     // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
    Family       string  // "qwen3", "llama3", etc.
    ParameterSize string // "8b", "32b", etc.
    Quantization string  // "Q4_K_M", "Q8_0", etc.
}

// ContextReporter ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æƒ…å ±ã‚’è¿”ã›ã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç”¨
type ContextReporter interface {
    GetContextWindow(ctx context.Context, model string) (int, error)
    GetTokenUsage(ctx context.Context) (*Usage, error)
}
```

---

## ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…

### 1. OpenAIäº’æ›ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆãƒ™ãƒ¼ã‚¹ï¼‰

å¤§åŠã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ã“ã‚Œã‚’å…±æœ‰ã™ã‚‹ã€‚å·®åˆ†ã ã‘ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã€‚

```go
// OpenAICompatProvider OpenAIäº’æ›APIã®ãƒ™ãƒ¼ã‚¹å®Ÿè£…
type OpenAICompatProvider struct {
    baseURL    string
    apiKey     string     // ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã¯ç©º
    model      string
    httpClient *http.Client
    info       ProviderInfo
}

// ã“ã‚Œ1ã¤ã§å¯¾å¿œ:
// - Ollama (/v1/chat/completions)
// - llama-server
// - llama.app
// - LM Studio
// - OpenAI
// - GLM / CodeGeeX
// - DeepSeek
// - Groq
// - Together AI
// - OpenRouter
// - Google Gemini (OpenAIäº’æ›ãƒ¢ãƒ¼ãƒ‰)
// - LocalAI
```

### 2. Ollama ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAIäº’æ› + ModelManageræ‹¡å¼µï¼‰

```go
// OllamaProvider = OpenAICompatProvider + Ollamaå›ºæœ‰æ©Ÿèƒ½
type OllamaProvider struct {
    OpenAICompatProvider          // åŸ‹ã‚è¾¼ã¿
    ollamaURL string              // Ollama APIç”¨URLï¼ˆ/api/...ï¼‰
}

// ModelManager ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’è¿½åŠ å®Ÿè£…
func (o *OllamaProvider) ListModels(ctx context.Context) ([]ModelInfo, error) { ... }
func (o *OllamaProvider) PullModel(ctx context.Context, name string) error { ... }
func (o *OllamaProvider) PullModelWithProgress(ctx context.Context, name string, progressFn PullProgressCallback) error { ... }
func (o *OllamaProvider) CheckModel(ctx context.Context, name string) (bool, error) { ... }
func (o *OllamaProvider) SearchModels(ctx context.Context, query string) ([]ModelInfo, error) { ... }
```

#### PullModelWithProgressï¼ˆé€²æ—è¡¨ç¤ºä»˜ããƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰

`/api/pull` ã‚’ `"stream": true` ã§å‘¼ã³å‡ºã—ã€JSON Lineså½¢å¼ã®é€²æ—æƒ…å ±ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å—ä¿¡ã™ã‚‹ã€‚
ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° `PullProgressCallback func(status string, completed, total int64)` ã‚’é€šã˜ã¦
å‘¼ã³å‡ºã—å…ƒã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒã‚¤ãƒˆæ•°ãƒ»ç·ãƒã‚¤ãƒˆæ•°ã‚’é€šçŸ¥ã™ã‚‹ã€‚

```
Ollama /api/pull ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹:
  {"status":"pulling manifest"}
  {"status":"pulling 9eba2761cf0b","digest":"sha256:...","total":4567890,"completed":1234567}
  {"status":"verifying sha256 digest"}
  {"status":"writing manifest"}
  {"status":"success"}
```

å‘¼ã³å‡ºã—å…ƒï¼ˆmain.goï¼‰ã§ã¯ `total > 0` ã®å ´åˆã«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º:
```
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  47.2% [1.9/4.0 GB]
```

#### ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‹è‡ªå‹•pullãƒ•ãƒ­ãƒ¼

ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ™‚ï¼ˆ`addLocalProvider`ï¼‰ã¨ç·¨é›†æ™‚ï¼ˆ`providerEdit`ï¼‰ã«ã€æ‰‹å‹•å…¥åŠ›ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åãŒ
Ollamaã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ç¢ºèªã—ã€æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®å ´åˆã¯ä»¥ä¸‹ã®é¸æŠè‚¢ã‚’æç¤º:

1. ä»Šã™ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ã `ollama pull`ï¼‰
2. ãã®ã¾ã¾è¨­å®šã‚’ä¿å­˜ï¼ˆå¾Œã§æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
3. æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é¸ã³ç›´ã™

èµ·å‹•æ™‚ã«ã‚‚ `pullModelIfNeeded()` ã§äºŒé‡ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã€ãƒ¢ãƒ‡ãƒ«ä¸åœ¨æ™‚ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯
æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚’æ¡ˆå†…ã™ã‚‹ã€‚

### 3. Anthropic ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆç‹¬è‡ªå®Ÿè£…ï¼‰

```go
// AnthropicProvider Claude APIç”¨
// ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å½¢å¼å¤‰æ›ãŒå¿…è¦
type AnthropicProvider struct {
    apiKey     string
    model      string     // "claude-sonnet-4-20250514" etc.
    httpClient *http.Client
}

// ChatRequest â†’ Anthropic Messages API å½¢å¼ã«å¤‰æ›
// - messages[].content â†’ content blocks
// - tools â†’ tool definitions (Anthropicå½¢å¼)
// - tool_calls â†’ tool_use blocks
// - tool results â†’ tool_result blocks
```

### 4. llama-server ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

```go
// LlamaServerProvider = OpenAICompatProviderï¼ˆãã®ã¾ã¾ï¼‰
// ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãªã—ã€‚èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã¯æŒ‡å®šæ¸ˆã¿ã€‚
// è‡ªå‹•æ¤œå‡º: localhost:8080 ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
type LlamaServerProvider struct {
    OpenAICompatProvider
}

// llama.app ã‚‚åŒã˜ï¼ˆå†…éƒ¨ã§llama-serverèµ·å‹•ï¼‰
// ãƒãƒ¼ãƒˆãŒç•°ãªã‚‹å¯èƒ½æ€§ã‚ã‚Š â†’ è‡ªå‹•æ¤œå‡ºã§ã‚«ãƒãƒ¼
```

---

## ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒã‚§ãƒ¼ãƒ³ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

### è¨­è¨ˆ

```go
// ProviderChain ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ããƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒã‚§ãƒ¼ãƒ³
type ProviderChain struct {
    providers []ChainEntry
    current   int
    mu        sync.RWMutex
}

type ChainEntry struct {
    Provider  LLMProvider
    Role      ChainRole    // Main, Sub, Fallback
    Priority  int          // ä½ã„å€¤ãŒå„ªå…ˆ
    Condition *ChainCondition // åˆ‡ã‚Šæ›¿ãˆæ¡ä»¶
}

type ChainRole string

const (
    RoleMain     ChainRole = "main"      // ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
    RoleSub      ChainRole = "sub"       // ã‚µãƒ–ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«åˆ¥ãƒ¢ãƒ‡ãƒ«ï¼‰
    RoleFallback ChainRole = "fallback"  // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ç­‰ï¼‰
)

type ChainCondition struct {
    OnError     bool   // ã‚¨ãƒ©ãƒ¼æ™‚ã«åˆ‡ã‚Šæ›¿ãˆ
    OnTimeout   bool   // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã«åˆ‡ã‚Šæ›¿ãˆ
    OnOverload  bool   // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¶…éæ™‚ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆå¤§ãã„ctxã®ãƒ¢ãƒ‡ãƒ«ã¸ï¼‰
    TaskType    string // ç‰¹å®šã‚¿ã‚¹ã‚¯æ™‚ã®ã¿åˆ‡ã‚Šæ›¿ãˆï¼ˆcode_reviewâ†’å¤§ãƒ¢ãƒ‡ãƒ«ç­‰ï¼‰
}
```

### ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œ

```
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    â”‚
    â–¼
[Main: Ollama qwen3:8b (ãƒ­ãƒ¼ã‚«ãƒ«)]
    â”‚ æˆåŠŸ â†’ å¿œç­”è¿”å´
    â”‚ å¤±æ•— â†“
    â–¼
[Sub: llama-server codestral:22b (ãƒ­ãƒ¼ã‚«ãƒ«åˆ¥ãƒ¢ãƒ‡ãƒ«)]
    â”‚ æˆåŠŸ â†’ å¿œç­”è¿”å´ + "âš  ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã§å¿œç­”" è¡¨ç¤º
    â”‚ å¤±æ•— â†“
    â–¼
[Fallback: OpenAI gpt-4o (ã‚¯ãƒ©ã‚¦ãƒ‰)]
    â”‚ æˆåŠŸ â†’ å¿œç­”è¿”å´ + "â˜ ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯" è¡¨ç¤º
    â”‚ å¤±æ•— â†“
    â–¼
ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºï¼ˆå…¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¤±æ•—ï¼‰
```

---

## è‡ªå‹•æ¤œå‡ºï¼ˆã‚¼ãƒ­ã‚³ãƒ³ãƒ•ã‚£ã‚°å¯¾å¿œï¼‰

åˆå¿ƒè€…å‘ã‘ï¼šä½•ã‚‚è¨­å®šã—ãªãã¦ã‚‚ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ã„ã¦ã„ã‚‹LLMã‚µãƒ¼ãƒãƒ¼ã‚’è‡ªå‹•æ¤œå‡ºã€‚

### æ¤œå‡ºé †åº

```go
// AutoDetect ãƒ­ãƒ¼ã‚«ãƒ«ã§èµ·å‹•ä¸­ã®LLMã‚µãƒ¼ãƒãƒ¼ã‚’è‡ªå‹•æ¤œå‡º
func AutoDetect(ctx context.Context) []DetectedProvider {
    var detected []DetectedProvider

    // ä¸¦è¡Œãƒã‚§ãƒƒã‚¯ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ2ç§’ï¼‰
    checks := []struct {
        name    string
        url     string
        check   func(ctx context.Context, url string) bool
    }{
        // 1. Ollama (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ 11434)
        {"ollama", "http://localhost:11434", checkOllama},

        // 2. llama-server / llama.app (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ 8080)
        {"llama-server", "http://localhost:8080", checkOpenAICompat},

        // 3. LM Studio (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ 1234)
        {"lm-studio", "http://localhost:1234", checkOpenAICompat},

        // 4. LocalAI (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ 8080)
        // llama-serverã¨ç«¶åˆã™ã‚‹ãŒã€/models ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§åˆ¤åˆ¥
        {"localai", "http://localhost:8080", checkLocalAI},

        // 5. ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ãƒˆ (ç’°å¢ƒå¤‰æ•° VIBE_LLM_URL)
        {"custom", os.Getenv("VIBE_LLM_URL"), checkOpenAICompat},
    }

    // å…¨ãƒãƒ¼ãƒˆä¸¦è¡Œãƒã‚§ãƒƒã‚¯
    for _, c := range checks {
        if c.url != "" && c.check(ctx, c.url) {
            detected = append(detected, DetectedProvider{
                Name: c.name,
                URL:  c.url,
            })
        }
    }

    return detected
}

func checkOllama(ctx context.Context, url string) bool {
    // GET /api/tags ãŒ 200 è¿”ã™ã‹
    resp, err := http.Get(url + "/api/tags")
    return err == nil && resp.StatusCode == 200
}

func checkOpenAICompat(ctx context.Context, url string) bool {
    // GET /v1/models ãŒ 200 è¿”ã™ã‹
    resp, err := http.Get(url + "/v1/models")
    return err == nil && resp.StatusCode == 200
}
```

### æ¤œå‡ºçµæœã®è¡¨ç¤ºï¼ˆåˆå¿ƒè€…å‘ã‘ï¼‰

```
ğŸ” LLMã‚µãƒ¼ãƒãƒ¼ã‚’æ¤œå‡ºä¸­...
  âœ“ Ollama (localhost:11434) - qwen3:8b, qwen3:32b ãŒåˆ©ç”¨å¯èƒ½
  âœ“ llama-server (localhost:8080) - ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿
  âœ— LM Studio (localhost:1234) - æœªæ¤œå‡º

â†’ Ollama (qwen3:8b) ã‚’ãƒ¡ã‚¤ãƒ³ã§ä½¿ç”¨ã—ã¾ã™
â†’ llama-server ã‚’ã‚µãƒ–ã§ä½¿ç”¨ã—ã¾ã™
```

---

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

### æ–°ã—ã„ Config æ§‹é€ 

```go
type Config struct {
    // === ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š ===
    Providers []ProviderConfig  // å„ªå…ˆé †ä½é †

    // === æ—§ Ollama è¨­å®šï¼ˆå¾Œæ–¹äº’æ›ï¼‰ ===
    OllamaHost string          // äº’æ›ç”¨ã€‚Providersæœªè¨­å®šæ™‚ã«ä½¿ç”¨

    // === å…±é€šè¨­å®š ===
    Model         string
    SidecarModel  string
    AutoModel     bool
    MaxTokens     int
    Temperature   float64
    ContextWindow int

    // ... (æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰)
}

type ProviderConfig struct {
    Name     string            `json:"name"`      // "ollama", "llama-server", "openai", etc.
    Type     string            `json:"type"`      // "local" or "cloud"
    URL      string            `json:"url"`       // ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    APIKey   string            `json:"api_key"`   // ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ï¼ˆç’°å¢ƒå¤‰æ•°å‚ç…§å¯ï¼‰
    Model    string            `json:"model"`     // ã“ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ä½¿ã†ãƒ¢ãƒ‡ãƒ«
    Role     string            `json:"role"`      // "main", "sub", "fallback"
    Priority int               `json:"priority"`  // ä½ã„ã»ã©å„ªå…ˆ
    Options  map[string]string `json:"options"`   // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³
}
```

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¾‹: `~/.config/vibe-local/config.json`

```json
{
  "providers": [
    {
      "name": "ollama",
      "type": "local",
      "url": "http://localhost:11434",
      "model": "qwen3:8b",
      "role": "main",
      "priority": 1
    },
    {
      "name": "llama-server",
      "type": "local",
      "url": "http://localhost:8080",
      "model": "",
      "role": "sub",
      "priority": 2,
      "options": {
        "note": "ãƒ¢ãƒ‡ãƒ«ã¯ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«æŒ‡å®šæ¸ˆã¿"
      }
    },
    {
      "name": "openai",
      "type": "cloud",
      "url": "https://api.openai.com",
      "api_key": "${OPENAI_API_KEY}",
      "model": "gpt-4o",
      "role": "fallback",
      "priority": 10
    }
  ]
}
```

### CLIãƒ•ãƒ©ã‚°ï¼ˆå¾Œæ–¹äº’æ› + æ–°è¦ï¼‰

```
# å¾Œæ–¹äº’æ›ï¼ˆæ—¢å­˜ï¼‰
--host <url>          Ollama URLï¼ˆ= --provider ollama --url <url> ã¨åŒã˜ï¼‰
--model <name>        ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«

# æ–°è¦
--provider <name>     ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æŒ‡å®š (ollama, llama-server, openai, anthropic, ...)
--url <url>           ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼URL
--api-key <key>       APIã‚­ãƒ¼ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ï¼‰
--fallback <name>     ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
--auto-detect         è‡ªå‹•æ¤œå‡ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: trueï¼‰
--list-providers      æ¤œå‡ºæ¸ˆã¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¸€è¦§
```

### ç’°å¢ƒå¤‰æ•°

```bash
# ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰
OLLAMA_HOST=http://localhost:11434
LLAMA_SERVER_URL=http://localhost:8080
LM_STUDIO_URL=http://localhost:1234

# ã‚¯ãƒ©ã‚¦ãƒ‰ API ã‚­ãƒ¼
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
DEEPSEEK_API_KEY=sk-...
GLM_API_KEY=...
GROQ_API_KEY=gsk_...
TOGETHER_API_KEY=...
OPENROUTER_API_KEY=sk-or-...

# æ±ç”¨
VIBE_LLM_URL=http://localhost:8080    # ã‚«ã‚¹ã‚¿ãƒ OpenAIäº’æ›ã‚µãƒ¼ãƒãƒ¼
VIBE_LLM_API_KEY=...                  # æ±ç”¨APIã‚­ãƒ¼
VIBE_PROVIDER=ollama                  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
```

---

## Anthropic ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¤‰æ›ä»•æ§˜

Anthropic ã ã‘ OpenAI äº’æ›ã§ã¯ãªã„ãŸã‚ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å¤‰æ›ãŒå¿…è¦ã€‚

### ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤‰æ›

```
OpenAIå½¢å¼ (å†…éƒ¨)              â†’  Anthropic Messages API
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
messages[].role: "system"      â†’  system ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«)
messages[].role: "user"        â†’  messages[].role: "user"
messages[].role: "assistant"   â†’  messages[].role: "assistant"
messages[].role: "tool"        â†’  messages[].content: [{type: "tool_result", ...}]

tools[].function               â†’  tools[].name, tools[].input_schema
tool_choice: "auto"            â†’  tool_choice: {type: "auto"}
tool_choice: "required"        â†’  tool_choice: {type: "any"}

temperature, max_tokens        â†’  ãã®ã¾ã¾
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹å¤‰æ›

```
Anthropic Messages API          â†’  OpenAIå½¢å¼ (å†…éƒ¨)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
content[].type: "text"          â†’  choices[0].message.content
content[].type: "tool_use"      â†’  choices[0].message.tool_calls[]
stop_reason: "end_turn"         â†’  choices[0].finish_reason: "stop"
stop_reason: "tool_use"         â†’  choices[0].finish_reason: "tool_calls"
usage.input_tokens              â†’  usage.prompt_tokens
usage.output_tokens             â†’  usage.completion_tokens
```

---

## XML ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã®çµ±åˆ

å°ã•ã„ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆ1.7bã€œ8bï¼‰ã¯ OpenAI å¼ function calling ã«å¯¾å¿œã—ã¦ã„ãªã„å ´åˆãŒã‚ã‚‹ã€‚
æ—¢å­˜ã® `xml_fallback.go` ã‚’ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ¬ãƒ™ãƒ«ã§çµ±åˆã€‚

```go
// OpenAICompatProvider ã® Chat ã§è‡ªå‹•åˆ¤å®š
func (p *OpenAICompatProvider) Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error) {
    resp, err := p.doRequest(ctx, req)
    if err != nil {
        return nil, err
    }

    // ãƒã‚¤ãƒ†ã‚£ãƒ– tool_calls ãŒè¿”ã£ã¦ããŸ â†’ ãã®ã¾ã¾è¿”ã™
    if len(resp.Choices) > 0 && len(resp.Choices[0].Message.ToolCalls) > 0 {
        return resp, nil
    }

    // ãƒ†ã‚­ã‚¹ãƒˆå¿œç­”ã®ã¿ â†’ XMLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§tool_callsæŠ½å‡ºã‚’è©¦ã¿ã‚‹
    if len(resp.Choices) > 0 && resp.Choices[0].Message.Content != "" {
        if req.Tools != nil && len(req.Tools) > 0 {
            knownTools := extractToolNames(req.Tools)
            calls, err := ExtractToolCallsFromText(resp.Choices[0].Message.Content, knownTools)
            if err == nil && len(calls) > 0 {
                resp.Choices[0].Message.ToolCalls = calls
                resp.Choices[0].FinishReason = "tool_calls"
            }
        }
    }

    return resp, nil
}
```

---

## å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º

### Phase 1: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æŠ½å‡ºï¼ˆç ´å£Šçš„å¤‰æ›´ãªã—ï¼‰

**ç›®æ¨™**: æ—¢å­˜å‹•ä½œã‚’å£Šã•ãšã«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å°å…¥

1. `LLMProvider` ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©
2. `OpenAICompatProvider` å®Ÿè£…ï¼ˆæ—¢å­˜ `Client` ã‚³ãƒ¼ãƒ‰ã‚’ç§»å‹•ï¼‰
3. `OllamaProvider` å®Ÿè£…ï¼ˆ`OpenAICompatProvider` åŸ‹ã‚è¾¼ã¿ + ãƒ¢ãƒ‡ãƒ«ç®¡ç†ï¼‰
4. `Agent.client *llm.Client` â†’ `Agent.provider llm.LLMProvider` ã«å¤‰æ›´
5. `main.go` ã§ `OllamaProvider` ã‚’ç”Ÿæˆã—ã¦æ¸¡ã™
6. æ—¢å­˜ãƒ†ã‚¹ãƒˆå…¨ãƒ‘ã‚¹ç¢ºèª

ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ:
```
internal/llm/
  provider.go           # LLMProvider ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©
  openai_compat.go      # OpenAIäº’æ›ãƒ™ãƒ¼ã‚¹å®Ÿè£…ï¼ˆæ—§ client.go + sync.go + streaming.goï¼‰
  ollama.go             # Ollamaå›ºæœ‰æ©Ÿèƒ½ï¼ˆæ—§ client.go ã®ãƒ¢ãƒ‡ãƒ«ç®¡ç†éƒ¨åˆ†ï¼‰
  chain.go              # ProviderChain
  xml_fallback.go       # (æ—¢å­˜)
```

### Phase 2: llama-server / llama.app å¯¾å¿œ

1. `LlamaServerProvider` å®Ÿè£…ï¼ˆ= `OpenAICompatProvider` ãã®ã¾ã¾ï¼‰
2. è‡ªå‹•æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ (`autodetect.go`)
3. `--provider` / `--url` CLIãƒ•ãƒ©ã‚°è¿½åŠ 
4. `config.json` ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šå¯¾å¿œ

### Phase 3: ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œ

1. APIã‚­ãƒ¼ç®¡ç†ï¼ˆç’°å¢ƒå¤‰æ•° + config.jsonã€`${ENV_VAR}` å±•é–‹ï¼‰
2. OpenAI / DeepSeek / GLM / Groq / Together / OpenRouter
   â†’ å…¨ã¦ `OpenAICompatProvider` ã® URL + APIKey å·®ã—æ›¿ãˆã§å¯¾å¿œ
3. `AnthropicProvider` å®Ÿè£…ï¼ˆç‹¬è‡ªå¤‰æ›ï¼‰

### Phase 4: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³

1. `ProviderChain` å®Ÿè£…
2. ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¶…éæ™‚ã®å¤§ãƒ¢ãƒ‡ãƒ«ã¸ã®åˆ‡ã‚Šæ›¿ãˆ
4. ãƒã‚§ãƒ¼ãƒ³çŠ¶æ…‹ã®UIè¡¨ç¤ºï¼ˆå³ãƒ‘ãƒãƒ« / ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒãƒ¼ï¼‰

### Phase 5: è‡ªå‹•æ¤œå‡ºï¼‹ã‚¼ãƒ­ã‚³ãƒ³ãƒ•ã‚£ã‚°

1. ãƒ­ãƒ¼ã‚«ãƒ«ã‚µãƒ¼ãƒãƒ¼ä¸¦è¡Œæ¤œå‡º
2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è‡ªå‹•è¨­å®š
3. æ¤œå‡ºçµæœã‹ã‚‰ã®è‡ªå‹•ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
4. åˆå¿ƒè€…å‘ã‘ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¦ã‚£ã‚¶ãƒ¼ãƒ‰

---

## å¾Œæ–¹äº’æ›æ€§

### æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å½±éŸ¿

```
å¤‰æ›´å‰: vibe --model qwen3:8b --host http://localhost:11434
å¤‰æ›´å¾Œ: vibe --model qwen3:8b --host http://localhost:11434  â† åŒã˜

--host ã¯å†…éƒ¨ã§ providers[0] = {name: "ollama", url: <host>} ã«å¤‰æ›ã€‚
config.json ã® "OllamaHost" ã‚‚å¼•ãç¶šãå‹•ä½œã€‚
```

### ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```json
// æ—§ config.json
{
  "OllamaHost": "http://localhost:11434",
  "Model": "qwen3:8b"
}

// â†’ å†…éƒ¨ã§è‡ªå‹•å¤‰æ› â†’

// æ–° config.json ç›¸å½“
{
  "providers": [
    {"name": "ollama", "url": "http://localhost:11434", "model": "qwen3:8b", "role": "main"}
  ]
}
```

---

## Agent å´ã®å¤‰æ›´ç‚¹

### Before

```go
type Agent struct {
    client *llm.Client    // Ollamaç›´æ¥ä¾å­˜
    ...
}

func NewAgent(client *llm.Client, ...) *Agent {
    return &Agent{client: client, ...}
}
```

### After

```go
type Agent struct {
    provider llm.LLMProvider  // ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä¾å­˜
    ...
}

func NewAgent(provider llm.LLMProvider, ...) *Agent {
    return &Agent{provider: provider, ...}
}

// callLLM ã¯å¤‰æ›´ãªã—ï¼ˆChatRequest/ChatResponse ã¯å…±é€šå‹ï¼‰
func (a *Agent) callLLM(ctx context.Context, messages []llm.Message, tools []llm.ToolDef) (*llm.ChatResponse, error) {
    req := &llm.ChatRequest{
        Model:       a.config.Model,
        Messages:    messages,
        Tools:       tools,
        Temperature: a.config.Temperature,
        MaxTokens:   a.config.MaxTokens,
    }
    return a.provider.Chat(ctx, req)  // client.ChatSync â†’ provider.Chat
}
```

---

## è¨­å®šä¾‹é›†

### ä¾‹1: Ollama ã®ã¿ï¼ˆåˆå¿ƒè€…ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

```bash
# ä½•ã‚‚è¨­å®šã—ãªãã¦ã‚‚è‡ªå‹•æ¤œå‡ºã§å‹•ã
vibe
```

### ä¾‹2: llama-server ã‚’ãƒ¡ã‚¤ãƒ³ã§ä½¿ã†

```bash
# llama-server èµ·å‹•æ¸ˆã¿ (localhost:8080)
vibe --provider llama-server --url http://localhost:8080
```

### ä¾‹3: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ã‚¤ãƒ³ + ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

```json
{
  "providers": [
    {"name": "ollama", "url": "http://localhost:11434", "model": "qwen3:8b", "role": "main"},
    {"name": "deepseek", "url": "https://api.deepseek.com", "api_key": "${DEEPSEEK_API_KEY}", "model": "deepseek-coder", "role": "fallback"}
  ]
}
```

### ä¾‹4: GLM / CodeGeeX ã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

```bash
vibe --provider glm --api-key $GLM_API_KEY --model glm-4-plus
```

### ä¾‹5: OpenRouter çµŒç”±ã§å¥½ããªãƒ¢ãƒ‡ãƒ«

```bash
vibe --provider openrouter --api-key $OPENROUTER_API_KEY --model anthropic/claude-sonnet-4
```

### ä¾‹6: ãƒ­ãƒ¼ã‚«ãƒ«2å°æ§‹æˆ

```json
{
  "providers": [
    {"name": "ollama", "url": "http://localhost:11434", "model": "qwen3:8b", "role": "main", "priority": 1},
    {"name": "ollama", "url": "http://192.168.1.100:11434", "model": "qwen3:32b", "role": "sub", "priority": 2}
  ]
}
```

---

## ã‚³ã‚¹ãƒˆãƒ»ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ï¼‰

```go
// CloudLimiter ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã‚³ã‚¹ãƒˆåˆ¶å¾¡
type CloudLimiter struct {
    MaxRequestsPerMinute int     // ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆ
    MaxTokensPerDay      int     // 1æ—¥ã‚ãŸã‚Šæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³
    MaxCostPerDay        float64 // 1æ—¥ã‚ãŸã‚Šæœ€å¤§ã‚³ã‚¹ãƒˆ (USD)
    WarnThreshold        float64 // è­¦å‘Šã—ãã„å€¤ (0.0-1.0)
}

// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã®æƒ³å®šå¤–èª²é‡‘ã‚’é˜²ã
var DefaultCloudLimiter = CloudLimiter{
    MaxRequestsPerMinute: 10,
    MaxTokensPerDay:      100000,
    MaxCostPerDay:        5.0,     // $5/day
    WarnThreshold:        0.8,     // 80%ã§è­¦å‘Š
}
```

åˆå¿ƒè€…ãŒçŸ¥ã‚‰ãšã«ã‚¯ãƒ©ã‚¦ãƒ‰ã§å¤§é‡èª²é‡‘ã•ã‚Œã‚‹ã®ã‚’é˜²ãï¼š
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç™ºå‹•æ™‚ã«ã€Œâ˜ ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¢ãƒ‡ãƒ«ã§å¿œç­”ã—ã¾ã™ï¼ˆAPIã‚³ã‚¹ãƒˆç™ºç”Ÿï¼‰ã€ã¨è¡¨ç¤º
- æ—¥æ¬¡ä¸Šé™åˆ°é”ã§è‡ªå‹•åœæ­¢ + ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º

---

## ã¾ã¨ã‚

### è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆ

1. **OpenAIäº’æ›ãŒãƒ™ãƒ¼ã‚¹**: 1ã¤ã®å®Ÿè£…ã§12+ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ã‚«ãƒãƒ¼
2. **Anthropicã ã‘ç‰¹åˆ¥å¯¾å¿œ**: å¤‰æ›ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å¸å
3. **æ®µéšçš„å®Ÿè£…**: Phase 1 ã§æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‚’å£Šã•ãšã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å°å…¥
4. **è‡ªå‹•æ¤œå‡ºå„ªå…ˆ**: åˆå¿ƒè€…ã¯ä½•ã‚‚è¨­å®šã—ãªãã¦ã‚ˆã„
5. **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³**: ãƒ­ãƒ¼ã‚«ãƒ«å¤±æ•— â†’ åˆ¥ãƒ­ãƒ¼ã‚«ãƒ« â†’ ã‚¯ãƒ©ã‚¦ãƒ‰
6. **ã‚³ã‚¹ãƒˆä¿è­·**: ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®æ„å›³ã—ãªã„èª²é‡‘ã‚’é˜²æ­¢

### å·¥æ•°è¦‹ç©ã‚Š

| Phase | å†…å®¹ | ç›®å®‰ |
|---|---|---|
| Phase 1 | ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹æŠ½å‡º | 2-3æ—¥ |
| Phase 2 | llama-server + è‡ªå‹•æ¤œå‡º | 1-2æ—¥ |
| Phase 3 | ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | 2-3æ—¥ |
| Phase 4 | ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ | 2æ—¥ |
| Phase 5 | ã‚¼ãƒ­ã‚³ãƒ³ãƒ•ã‚£ã‚° + ã‚¦ã‚£ã‚¶ãƒ¼ãƒ‰ | 1-2æ—¥ |
| **åˆè¨ˆ** | | **8-12æ—¥** |
